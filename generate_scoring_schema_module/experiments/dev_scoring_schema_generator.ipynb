{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "\n",
    "from google import genai\n",
    "\n",
    "from cleanlab_tlm import TLM\n",
    "\n",
    "from athina_logger.api_key import AthinaApiKey\n",
    "from athina_logger.inference_logger import InferenceLogger\n",
    "from athina_logger.exception.custom_exception import CustomException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_jd_path = \"../data/raw/jd/\"\n",
    "\n",
    "# List out names of all the files in the directory\n",
    "filenames = os.listdir(folder_jd_path)\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_descriptions = []\n",
    "\n",
    "# Read the contents of each file\n",
    "for filename in filenames:\n",
    "    with open(folder_jd_path + filename, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        job_descriptions.append(content)\n",
    "        \n",
    "print(job_descriptions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_keys_folder = \"../secret_keys/\"\n",
    "\n",
    "# Read the Gemini API key\n",
    "with open(secret_keys_folder + \"gemini_api_key.txt\", 'r', encoding='utf-8') as file:\n",
    "    gemini_api_key = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder containing the prompt templates\n",
    "prompt_template_folder = \"./\"\n",
    "\n",
    "# List all files in the prompt template folder\n",
    "prompt_files = os.listdir(prompt_template_folder)\n",
    "\n",
    "# Filter out files that match the pattern \"prompt_template_v*.txt\"\n",
    "prompt_files = [f for f in prompt_files if re.match(r\"prompt_template_v\\d+\\.txt\", f)]\n",
    "\n",
    "# Sort the files by version number in descending order\n",
    "prompt_files.sort(key=lambda x: int(re.search(r'\\d+', x).group()), reverse=True)\n",
    "\n",
    "# Get the prompt template with the highest version number\n",
    "latest_prompt_template_file = prompt_files[0]\n",
    "\n",
    "# Read the content of the latest prompt template file\n",
    "with open(prompt_template_folder + latest_prompt_template_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    prompt_template = file.read().strip()\n",
    "\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(prompt_template: str, scoring_scale_min: int,\n",
    "               scoring_scale_max: int, job_description: str):\n",
    "    \"\"\"\n",
    "    Generates a prompt by filling in the provided template with the given parameters.\n",
    "\n",
    "    Args:\n",
    "        prompt_template (str): The template string for the prompt.\n",
    "        scoring_scale_min (int): The minimum value on the scoring scale (e.g., 1).\n",
    "        scoring_scale_max (int): The maximum value on the scoring scale (e.g., 5).\n",
    "        screening_phase (str): The phase this schema is for (e.g., \"Initial Application/Resume Screening\").\n",
    "        job_description (str): The job description to be included in the prompt.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated prompt with the parameters filled in.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the minimum score is not less than the maximum score.\n",
    "    \"\"\"\n",
    "    # Check if the minimum score is less than the maximum score\n",
    "    if scoring_scale_min >= scoring_scale_max:\n",
    "        raise Exception(\"The min score of the scale must be less than the max score.\")\n",
    "    \n",
    "    # Replace single braces with double braces in the template first\n",
    "    escaped_template = prompt_template.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    \n",
    "    # But restore the actual format placeholders\n",
    "    escaped_template = escaped_template.replace(\"{{scoring_scale_min}}\", \"{scoring_scale_min}\")\n",
    "    escaped_template = escaped_template.replace(\"{{scoring_scale_max}}\", \"{scoring_scale_max}\")\n",
    "    escaped_template = escaped_template.replace(\"{{job_description}}\", \"{job_description}\")\n",
    "    \n",
    "    # Fill the prompt template with values of parameters\n",
    "    prompt = escaped_template.format(\n",
    "        scoring_scale_min=scoring_scale_min,\n",
    "        scoring_scale_max=scoring_scale_max,\n",
    "        job_description=job_description\n",
    "    )\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_scale_min = 1\n",
    "scoring_scale_max = 5\n",
    "sample_job_description = job_descriptions[0]\n",
    "\n",
    "prompt = get_prompt(prompt_template=prompt_template,\n",
    "                    scoring_scale_min=scoring_scale_min,\n",
    "                    scoring_scale_max=scoring_scale_max,\n",
    "                    job_description=sample_job_description)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gemini-2.0-flash'\n",
    "client = genai.Client(api_key=gemini_api_key)\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt,\n",
    ")\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "# Calculate the response time\n",
    "response_time = end_time - start_time\n",
    "\n",
    "# Use the response as a JSON string.\n",
    "print(response.text)\n",
    "print(f\"Response Time: {response_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_json_string = response.text.strip().removeprefix('```json\\n').removesuffix('\\n```')\n",
    "clean_json_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_schema = json.loads(clean_json_string)\n",
    "scoring_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(scoring_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_valid_schema(scoring_schema: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Validates a scoring schema dictionary.\n",
    "\n",
    "    Checks performed:\n",
    "    1.  Input is a dictionary.\n",
    "    2.  Attempts to find a list containing dictionary elements (assumed scoring categories).\n",
    "    3.  Attempts to find a key within those dictionaries representing weight percentage\n",
    "        (looks for keys containing 'weight' or 'percent' with numeric values).\n",
    "    4.  Checks if the sum of these numeric weight percentages is close to 100.\n",
    "    5.  Handles potential missing keys, non-list/dict structures, and non-numeric weights gracefully.\n",
    "\n",
    "    Args:\n",
    "        scoring_schema: The dictionary representing the parsed JSON scoring schema.\n",
    "\n",
    "    Returns:\n",
    "        True if the schema appears valid (primarily based on weights summing to 100\n",
    "        and basic structure being identifiable), False otherwise.\n",
    "    \"\"\"\n",
    "    print(f\"--- Running Validation ---\") # Optional: for debugging\n",
    "\n",
    "    # 1. Basic Input Type Check\n",
    "    if not isinstance(scoring_schema, dict):\n",
    "        print(\"Validation Failed: Input is not a dictionary.\")\n",
    "        return False\n",
    "    if not scoring_schema:\n",
    "        print(\"Validation Failed: Input dictionary is empty.\")\n",
    "        return False\n",
    "\n",
    "    category_list = None\n",
    "    category_list_key = None\n",
    "\n",
    "    # 2. Find the list containing category dictionaries\n",
    "    for key, value in scoring_schema.items():\n",
    "        if isinstance(value, list):\n",
    "            # Check if the list primarily contains dictionaries\n",
    "            if value and all(isinstance(item, dict) for item in value):\n",
    "                 # Assume the first such list found is the one we want\n",
    "                category_list = value\n",
    "                category_list_key = key\n",
    "                print(f\"Found candidate category list under key: '{category_list_key}'\") # Optional\n",
    "                break\n",
    "            elif not value: # It's an empty list\n",
    "                 # Could be the category list, but empty. Handle later.\n",
    "                 category_list = value\n",
    "                 category_list_key = key\n",
    "                 print(f\"Found candidate category list (empty) under key: '{category_list_key}'\") # Optional\n",
    "                 break # Treat empty list as potential category list for now\n",
    "\n",
    "    if category_list is None:\n",
    "        print(\"Validation Failed: Could not find a list containing dictionaries (potential scoring categories).\")\n",
    "        return False\n",
    "\n",
    "    # Handle case where the identified list is empty\n",
    "    if not category_list:\n",
    "        print(\"Validation Failed: The identified category list is empty. Schema requires categories.\")\n",
    "        # Assuming an empty schema isn't valid if weights are expected.\n",
    "        # If a schema with 0 categories summing to 0 *is* valid, change this logic.\n",
    "        return False\n",
    "\n",
    "    weight_key = None\n",
    "    # 3. Find the weight percentage key within the first category dictionary\n",
    "    first_category = category_list[0] # We know the list is not empty here\n",
    "    if isinstance(first_category, dict):\n",
    "        for key, value in first_category.items():\n",
    "            # Heuristic: key name suggests weight AND value is numeric\n",
    "            key_lower = key.lower()\n",
    "            if ('weight' in key_lower or 'percent' in key_lower) and isinstance(value, (int, float)):\n",
    "                weight_key = key\n",
    "                print(f\"Found candidate weight key: '{weight_key}'\") # Optional\n",
    "                break # Assume the first match is the one we want\n",
    "\n",
    "    if weight_key is None:\n",
    "        print(f\"Validation Failed: Could not find a suitable weight key (containing 'weight' or 'percent' with a numeric value) in the first item of list '{category_list_key}'.\")\n",
    "        return False\n",
    "\n",
    "    # 4. Calculate the sum of weights\n",
    "    total_weight = 0.0\n",
    "    found_items = 0\n",
    "    for index, item in enumerate(category_list):\n",
    "        if not isinstance(item, dict):\n",
    "            print(f\"Validation Failed: Item at index {index} in list '{category_list_key}' is not a dictionary.\")\n",
    "            return False\n",
    "\n",
    "        weight_value = item.get(weight_key) # Use .get() for safety\n",
    "\n",
    "        if weight_value is None:\n",
    "            # Treat missing weight key in an item as invalid\n",
    "            print(f\"Validation Failed: Weight key '{weight_key}' missing in item at index {index} of list '{category_list_key}'.\")\n",
    "            return False\n",
    "\n",
    "        if not isinstance(weight_value, (int, float)):\n",
    "             # Treat non-numeric weight as invalid\n",
    "            print(f\"Validation Failed: Weight value '{weight_value}' for key '{weight_key}' in item at index {index} is not numeric.\")\n",
    "            return False\n",
    "\n",
    "        total_weight += float(weight_value) # Convert to float for consistent summation\n",
    "        found_items += 1\n",
    "\n",
    "    print(f\"Calculated total weight: {total_weight} from {found_items} items.\") # Optional\n",
    "\n",
    "    # 5. Check if the sum is close to 100\n",
    "    # Use math.isclose for robust floating-point comparison\n",
    "    # abs_tol=1e-9 is a small tolerance for potential floating point inaccuracies\n",
    "    is_sum_valid = math.isclose(total_weight, 100.0, abs_tol=1e-9)\n",
    "\n",
    "    if not is_sum_valid:\n",
    "        print(f\"Validation Failed: Total weight {total_weight} is not equal to 100.\")\n",
    "        return False\n",
    "\n",
    "    print(\"--- Validation Successful ---\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_validity = check_valid_schema(scoring_schema=scoring_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Cleanlab TLM API key\n",
    "with open(secret_keys_folder + \"cleanlab_tlm_api_key.txt\", 'r', encoding='utf-8') as file:\n",
    "    cleanlab_tlm_api_key = file.read().strip()\n",
    "os.environ[\"CLEANLAB_TLM_API_KEY\"] = cleanlab_tlm_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = TLM(options={\"log\": [\"explanation\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm_response = tlm.get_trustworthiness_score(prompt, response=str(scoring_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "trustworthiness_score = tlm_response['trustworthiness_score']\n",
    "trustworthiness_explanation = tlm_response['log']['explanation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Athina API key from the file\n",
    "with open(secret_keys_folder + \"athina_api_key.txt\", 'r', encoding='utf-8') as file:\n",
    "    athina_api_key = file.read().strip()\n",
    "\n",
    "# Set Athina API Key\n",
    "AthinaApiKey.set_api_key(athina_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  InferenceLogger.log_inference(\n",
    "      prompt_slug=\"npo_hr_screening_schema_writer_jd\",\n",
    "      prompt=prompt,\n",
    "      language_model_id=model,\n",
    "      response=str(scoring_schema),\n",
    "      response_time=response_time,\n",
    "      custom_attributes={\n",
    "          \"response_validity\": response_validity,\n",
    "          \"trustworthiness_score\": trustworthiness_score,\n",
    "          \"trustworthiness_explanation\": trustworthiness_explanation\n",
    "      }\n",
    "  )\n",
    "except Exception as e:\n",
    "  if isinstance(e, CustomException):\n",
    "      print(e.status_code)\n",
    "      print(e.message)\n",
    "  else:\n",
    "      print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_model_lists = [\n",
    "    \"gemini-2.5-pro-exp-03-25\",\n",
    "    \"gemini-2.0-flash\",\n",
    "    \"gemini-2.0-flash-lite\",\n",
    "    \"gemini-1.5-flash\",\n",
    "    \"gemini-1.5-flash-8b\",\n",
    "    \"gemini-1.5-pro\"\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_in_hr-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
